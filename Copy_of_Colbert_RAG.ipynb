{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWxf1kEwBI2X"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader(\"data/rel18\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX4mMimbEwKL"
      },
      "outputs": [],
      "source": [
        "docs_str = []\n",
        "for doc in documents:\n",
        "  docs_str.append(doc.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmE5imB1BPQf"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meIQrT0CEqqP"
      },
      "outputs": [],
      "source": [
        "len(docs_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4K3s5zwBOWq"
      },
      "outputs": [],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "RAG.index(\n",
        "    collection=docs_str,\n",
        "    index_name=\"ITU RAG 150\",\n",
        "    max_document_length=150,\n",
        "    split_documents=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCy1mJFeEnZx"
      },
      "outputs": [],
      "source": [
        "results = RAG.search(query=\"What does the UE provide to the AS for slice aware cell reselection?\", k=7)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAEVqbUwOqAf"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "config = PeftConfig.from_pretrained(\"alexgichamba/phi-2-finetuned-qa-lora-r32-a16_notag\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\").to('cuda')\n",
        "model = PeftModel.from_pretrained(base_model, \"alexgichamba/phi-2-finetuned-qa-lora-r32-a16_notag\").to('cuda')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icNQHFtrZX6z"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# Read questions from the JSON file\n",
        "with open('data/366qs.txt', 'r') as file1:\n",
        "  with open('data/questions_new.txt', 'r') as file2:\n",
        "    questions = json.load(file1)\n",
        "    # questions.update(json.load(file2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGRcaU1imf7G"
      },
      "outputs": [],
      "source": [
        "first_key = next(iter(questions))\n",
        "first_value = questions[first_key]\n",
        "first_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thTVfSHBnL1-"
      },
      "outputs": [],
      "source": [
        "options = [(k, v) for k, v in first_value.items() if k.startswith(\"option\")]\n",
        "options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYMHsnlAomDc"
      },
      "outputs": [],
      "source": [
        "res = RAG.search(query=first_value['question'], k=7)\n",
        "len(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(first_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCKsjWaYEF1H"
      },
      "outputs": [],
      "source": [
        "def create_prompt(question, options, context, abbreviations):\n",
        "    options_text = \"\\n\".join([f\"Option {i+1}: {opt[1]}\" for i, opt in enumerate(options)])\n",
        "    # abbreviations is a list of dictionaries of form {\"abbreviation\": \"full form\"}\n",
        "    abbreviations_text = \"\\n\".join([f\"{list(abbrev.keys())[0]}: {list(abbrev.values())[0]}\" for abbrev in abbreviations])\n",
        "    prompt = (\n",
        "        f\"Instruct: You will answer each question correctly by giving only the Option ID, the number that follows each Option.\\n\"\n",
        "        f\"The output should be in the format: Option <Option id>\\n\"\n",
        "        f\"Provide the answer to the following multiple choice question in the specified format.\\n\\n\"\n",
        "        f\"Context:\\n{context}\\n\\n\"\n",
        "        f\"Abbreviations:\\n{abbreviations_text}\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Options:\\n{options_text}\\n\"\n",
        "        f\"Answer: Option\"\n",
        "    )\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgh1xsqqmbRY"
      },
      "outputs": [],
      "source": [
        "def generate_answer(question, options, context, abbreviations, model, tokenizer):\n",
        "    prompt = create_prompt(question, options, context, abbreviations)\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
        "\n",
        "    # Ensure the pad token is set\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    attention_mask = input_ids.ne(tokenizer.pad_token_id).long().to('cuda')  # Set attention mask\n",
        "\n",
        "    # Generate the answer with appropriate parameters\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=10,  # Limit the number of new tokens generated\n",
        "        pad_token_id=tokenizer.eos_token_id,  # Handle padding correctly\n",
        "        num_beams=1,  # Use beam search to improve quality of generated answers\n",
        "        early_stopping=True  # Stop early when enough beams have reached EOS\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    # print(f\"RESPONSE - {answer[900:]}\")\n",
        "    # print(\"-------------------------------------------\")\n",
        "    print(f\"Generated answer: {answer}\")\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from data.prepare_docs import find_appearing_abbreviations\n",
        "print(find_appearing_abbreviations(first_value))\n",
        "type(find_appearing_abbreviations(first_value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPt0i4jZmc9D"
      },
      "outputs": [],
      "source": [
        "ans = generate_answer(first_value['question'], options, \" \".join([result['content'] for result in results]), find_appearing_abbreviations(first_value), model, tokenizer)\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc7xSkWcsU_8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# First search for the full pattern\n",
        "def parse_answer(response):\n",
        "  match = re.search(r'Answer:\\s*Option\\s*(\\d+)', response, re.IGNORECASE)\n",
        "  if match:\n",
        "      answer = f\"Option {match.group(1)}\"\n",
        "  else:\n",
        "      # Try another pattern if the first one fails\n",
        "      match = re.search(r'(\\d+)', response, re.IGNORECASE)\n",
        "      if match:\n",
        "          answer = f\"Option {match.group(1)}\"\n",
        "      else:\n",
        "          answer = \"Error\"\n",
        "  return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMQbWBHCPd-x"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "responses = []\n",
        "\n",
        "# Loop through each question and get the response\n",
        "for q_id, q_data in tqdm(questions.items(), desc=\"Processing questions\"):\n",
        "    q_id_number = q_id.split()[1]\n",
        "    question_text = q_data[\"question\"]\n",
        "    question_text = re.sub(r'\\s*\\[.*?\\]\\s*$', '', question_text)\n",
        "    # options = [v for k, v in q_data.items() if k.startswith(\"option\")]\n",
        "    options = [(k, v) for k, v in q_data.items() if k.startswith(\"option\")]\n",
        "\n",
        "    # Retrieve context using ColBERT search\n",
        "    results = RAG.search(query=question_text, k=7)\n",
        "    context = \" \".join([result['content'] for result in results])\n",
        "\n",
        "    abbreviations = find_appearing_abbreviations(q_data)\n",
        "    # Generate the answer using the loaded model\n",
        "    response = generate_answer(question_text, options, context, abbreviations, model, tokenizer)\n",
        "\n",
        "    answer = parse_answer(response)\n",
        "\n",
        "    # Extract the answer ID from the response\n",
        "    match = re.search(r'Option (\\d+)', answer)\n",
        "    if match:\n",
        "        try:\n",
        "            answer_id = int(match.group(1))\n",
        "            print(f\"Answer ID: {answer_id}\")\n",
        "            responses.append([q_id_number, answer_id, \"Phi-2\"])\n",
        "        except (KeyError, IndexError, ValueError) as e:\n",
        "            responses.append([q_id_number, \"Error\", \"Phi-2\"])\n",
        "            print(f\"Error processing question {q_id}: {answer}\")\n",
        "    else:\n",
        "        responses.append([q_id_number, \"Error\", \"Phi-2\"])\n",
        "        print(f\"Error processing question {q_id_number}: {answer}\")\n",
        "\n",
        "# Save responses to a CSV file\n",
        "with open('output_results.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow([\"Question_ID\", \"Answer_ID\", \"Task\"])\n",
        "    csvwriter.writerows(responses)\n",
        "\n",
        "print(\"Processing complete. Responses saved to 'output_results.csv'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "grade the 366 qs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub7HWTlivGrd"
      },
      "outputs": [],
      "source": [
        "# Load the questions dataset from the JSON file\n",
        "with open('data/366qs.txt', 'r') as rubric:\n",
        "    qs_w_ans = json.load(rubric)\n",
        "\n",
        "# Load the responses from the CSV file\n",
        "responses = []\n",
        "with open('output_results.csv', 'r') as answers:\n",
        "    reader = csv.DictReader(answers)\n",
        "    for row in reader:\n",
        "        responses.append(row)\n",
        "\n",
        "# Initialize score\n",
        "correct_answers = 0\n",
        "total_questions = len(responses)\n",
        "\n",
        "# track question_ids for failed questions\n",
        "failed_questions = []\n",
        "# Compare the responses with the correct answers\n",
        "for response in responses:\n",
        "    question_id = response['Question_ID']\n",
        "    answer_id = response['Answer_ID']\n",
        "    task = response['Task']\n",
        "    \n",
        "    # Find the corresponding question in the JSON data\n",
        "    question_key = f\"question {question_id}\"\n",
        "    if question_key in qs_w_ans:\n",
        "        correct_answer = qs_w_ans[question_key]['answer']\n",
        "        # Extract the correct option number from the correct answer string\n",
        "        correct_option_number = correct_answer.split()[1].replace(\":\", \"\")\n",
        "        \n",
        "        # Check if the given answer matches the correct answer\n",
        "        if answer_id == correct_option_number:\n",
        "            correct_answers += 1\n",
        "        else:\n",
        "            # append questionid and answerid to failed questions\n",
        "            failed_questions.append((question_id, answer_id))\n",
        "\n",
        "# Calculate the score\n",
        "score = (correct_answers / total_questions) * 100\n",
        "\n",
        "# Print the results\n",
        "print(f\"Total Questions: {total_questions}\")\n",
        "print(f\"Correct Answers: {correct_answers}\")\n",
        "print(f\"Score: {score:.2f}%\")\n",
        "# write failed questions to a file\n",
        "with open('failed_questions.txt', 'w') as file:\n",
        "    for question_id, answer_id in failed_questions:\n",
        "        file.write(f\"{question_id} {answer_id}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def append_dummy_data(csv_filename):\n",
        "    dummy_task = \"Phi-2\"\n",
        "    dummy_entries = [(dummy_id, 0, dummy_task) for dummy_id in range(10000, 12000)]\n",
        "\n",
        "    try:\n",
        "        # Open the existing CSV file and append dummy data\n",
        "        with open(csv_filename, \"a\", newline='') as csvfile:\n",
        "            csv_writer = csv.writer(csvfile)\n",
        "            for entry in dummy_entries:\n",
        "                csv_writer.writerow(entry)\n",
        "        print(\"Dummy data has been appended to the CSV file.\")\n",
        "    except Exception as e:\n",
        "        print(\"Encountered an error while appending dummy data.\")\n",
        "        print(e)\n",
        "\n",
        "# Assuming 'output_results.csv' is the CSV file to which we need to append dummy data\n",
        "csv_filename = \"output_results.csv\"\n",
        "append_dummy_data(csv_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate static context for training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing questions: 100%|██████████| 1400/1400 [00:40<00:00, 34.56it/s]\n"
          ]
        }
      ],
      "source": [
        "# training set file\n",
        "training_set_file = \"data/qs_train.txt\"\n",
        "output_file = \"data/qs_train_with_context.txt\"\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "# training_set_file = \"data/366qs.txt\"\n",
        "# output_file = \"data/366qs_with_context.txt\"\n",
        "\n",
        "with open(training_set_file, 'r') as file:\n",
        "    questions = json.load(file)\n",
        "\n",
        "for q_id, q_data in tqdm(questions.items(), desc=\"Processing questions\"):\n",
        "    q_id_number = q_id.split()[1]\n",
        "    question_text = q_data[\"question\"]\n",
        "    question_text = re.sub(r'\\s*\\[.*?\\]\\s*$', '', question_text)\n",
        "    results = RAG.search(query=question_text, k=3)\n",
        "    context = \" \".join([result['content'] for result in results])\n",
        "    q_data[\"context\"] = context\n",
        "\n",
        "with open(output_file, 'w') as file:\n",
        "    json.dump(questions, file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1640\n"
          ]
        }
      ],
      "source": [
        "results = RAG.search(query=\"What does the NEF notify to the AF after determining the suitable DNAI(s)?\", k=3)\n",
        "results_exp = \" \".join([result['content'] for result in results])\n",
        "print(len(results_exp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
